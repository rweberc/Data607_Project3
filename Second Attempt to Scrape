#More attempts at making the scraping process work 

library(XML)
library(rvest)
library(RCurl)
#library(xlsx)
library(stringr)
library(tidyr)
library(urltools)
library(dplyr)
library(plotly)
library(ggplot2)

#This was an attempt to extract topjob titles and summaries using the search results in ideed
datajobs <- lapply(paste0('https://www.indeed.com/jobs?q=data+science&l=New+York+State', 1:100),
                function(url){
                    url %>% read_html() %>% 
                        html_nodes(".summary , #sja1") %>% 
                        html_text()
                })
                
  
  #This is an attempt to improve the method above 
  library(rvest)
library(purrr)

url_base <-"https://www.indeed.com/jobs?q=data+science&l=New+York+State"

map_df(1:10, function(i) {

  # simple but effective progress indicator
  cat(".")

  pg <- read_html(sprintf(url_base, i))

  data.frame(summary=unlist(html_text(html_nodes(pg, ".summary"))),
             company=unlist(html_text(html_nodes(pg, ".company"))),  #.rating
             location=html_text(html_nodes(pg, ".location")),
             company_rating=gsub(" Points", "", html_text(html_nodes(pg, ".rating"))),
             stringsAsFactors=FALSE)

}) -> datajobs

dplyr::glimpse(datajobs)
